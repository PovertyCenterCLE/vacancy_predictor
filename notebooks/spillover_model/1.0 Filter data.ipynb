{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data used in vacancy model based on TCI parcel numbers\n",
    "\n",
    "Parcels surveyed in Summer 2015 so all data pulled should come from before that. Goal of script/notebook is to filter datasets by the parcel numbers in the TCI survey, although we will filter again based on existence of structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Dropbox/largetransfer/luc/carter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/'.join(os.getcwd().split('/')[:-1])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci = pd.read_excel(path+'/data/inspection_data/Cleveland_Final_Results_Table_FOR_DISTRIBUTION_20151111.xlsx', encoding=\"ISO-8859-1\") \n",
    "\n",
    "def get_vacant(x):\n",
    "    if x == 'Occupied Structure':\n",
    "        return 0\n",
    "    elif x == 'Vacant Structure':\n",
    "        return 1\n",
    "    else: \n",
    "        return -1\n",
    "    \n",
    "tci['vacant'] = tci['Survey Category'].apply(get_vacant)\n",
    "tci['parcel'] = tci.PIN.apply(lambda x: x[0:3]+'-'+x[3:5]+'-'+x[5:])\n",
    "\n",
    "# tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].to_csv(path+'/data/model_data/tci_1_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12179"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tci[tci.vacant>0].vacant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131522, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tci[(tci.USE_CLASS=='R')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci = pd.read_csv(path+'/data/model_data/tci_1_0.csv', parse_dates=['Date'], dtype={'Ward':object,'PIN':str})\n",
    "ppns = set(tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tracts = pd.read_csv(path+'/data/original_data/clv_par_census.csv')\n",
    "demo = pd.read_csv(path+'/data/original_data/sociodemographic_Data.csv')\n",
    "\n",
    "tracts = pd.merge(tracts, demo, left_on='NAME10', right_on='Census Tract', how='left')\n",
    "cols = [0,1,5,7,9,11,13,14,15,16,17,18,20,22,24,26,28,30,32,34,36,38,40]\n",
    "tracts.iloc[:,cols].to_csv(path+'/data/clean_data/demographic.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path+'/data/original_data/main_prop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbf = simpledbf.Dbf5(path+'/data/original_data/parcel0611_lookup2010.dbf').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178548, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbf[dbf.CITY=='CLEVELAND'].groupby('PARCEL').first().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# infile = path+'/data/original_data/main_prop.csv'\n",
    "# outfile = path+'/data/clean_data/main_prop13.csv'\n",
    "\n",
    "# with open(infile, 'r', encoding=\"ISO-8859-1\") as fin, open(outfile, 'w') as fout:\n",
    "#     write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#     header = next(csv.reader(fin))\n",
    "#     write_to.writerow(header)\n",
    "#     for row in csv.reader(fin):\n",
    "#         if row[0] in ppns:\n",
    "#             write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commented code only needs to be run once\n",
    "\n",
    "main = pd.read_csv(path+'/data/clean_data/main_prop13.csv', dtype=object)\n",
    "\n",
    "main = main.drop_duplicates()\n",
    "main = main[main.parcel.isin(ppns)]\n",
    "\n",
    "main14 = main[main.taxyr=='2014'].groupby('parcel').first().reset_index()\n",
    "main13 = main[main.taxyr=='2013'].groupby('parcel').first().reset_index()\n",
    "\n",
    "print(main13.shape, main14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main13 = main13.set_index('parcel')\n",
    "main13 = main13.rename(columns={'condition':'condition13'})\n",
    "main14 = main14.rename(columns={'condition':'condition14'})\n",
    "pd.merge(main14, main13[['condition13']], how='left', left_on='parcel', right_index=True)\\\n",
    "    .to_csv(path+'/data/clean_data/main_prop_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residential characteristics \n",
    "Filename: ```res2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425302, 46)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('parcel').count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first part only needs to be run once\n",
    "\n",
    "res = pd.read_csv(path+'/data/original_data/res/res2014.csv')\n",
    "# res = res[res.parcel.isin(ppns)]\n",
    "# res.to_csv(path+'/data/clean_data/res.csv', index=False)\n",
    "\n",
    "# res = pd.read_csv(path+'/data/clean_data/res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax bill\n",
    "Filename: ```dec14_tci.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (4,20,22,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_csv(path+'/data/clean_data/taxbill_may15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/sep14.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_sep14.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## County land bank\n",
    "Filename: ```count_land_bank.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = pd.read_csv(path+'/data/original_data/count_land_bank.csv', parse_dates=[3,4])\n",
    "\n",
    "lb = lb[lb['parcel'].isin(ppns)]\n",
    "# lb = lb[lb['acq_dt']<np.datetime64('2015-06-01')]\n",
    "\n",
    "lb.to_csv(path+'/data/clean_data/county_lb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreclosure filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_beyond.csv', parse_dates = [2])\n",
    "\n",
    "fc = fc[fc['parcel'].isin(ppns)]\n",
    "# fc = fc[fc['filedate']<np.datetime64('2015-06-01')]\n",
    "\n",
    "fc.to_csv(path+'/data/clean_data/foreclosure_filings2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148           11516 EDGEWATER DRIVE\n",
       "1149           11436 EDGEWATER DRIVE\n",
       "1150           11315 EDGEWATER DRIVE\n",
       "1151           11425 EDGEWATER DRIVE\n",
       "1152           11439 EDGEWATER DRIVE\n",
       "1153               11312 LAKE AVENUE\n",
       "1154           11015 EDGEWATER DRIVE\n",
       "1155               11110 LAKE AVENUE\n",
       "1156               11018 LAKE AVENUE\n",
       "1157             10710 EDGEWATER DR.\n",
       "1158             10710 EDGEWATER DR.\n",
       "1159           10700 EDGEWATER DRIVE\n",
       "1160                  2402 CLOFAX RD\n",
       "1161           10801 EDGEWATER DRIVE\n",
       "1162               10900 LAKE AVENUE\n",
       "1163               10806 LAKE AVENUE\n",
       "1164               10416 LAKE AVENUE\n",
       "1165               10001 CLIFF DRIVE\n",
       "1166               10001 CLIFF DRIVE\n",
       "1167               10001 CLIFF DRIVE\n",
       "1168               10001 CLIFF DRIVE\n",
       "1169               10001 CLIFF DRIVE\n",
       "1170                  9995 CLIFF DR.\n",
       "1171                  9995 CLIFF DR.\n",
       "1172           10217 EDGEWATER DRIVE\n",
       "1173           10001 EDGEWATER DRIVE\n",
       "1174               10010 LAKE AVENUE\n",
       "1175                  1323 W 95TH ST\n",
       "1176           1339 WEST 95TH STREET\n",
       "1179           1371 WEST 95TH STREET\n",
       "                     ...            \n",
       "76575              6008 ARCHMERE AVE\n",
       "76576              6008 ARCHMERE AVE\n",
       "76594             6024 DELORA AVENUE\n",
       "76595             6024 DELORA AVENUE\n",
       "76639                6000 IRA AVENUE\n",
       "76640                6000 IRA AVENUE\n",
       "76641                   6006 IRA AVE\n",
       "90897            13796 LAKE SHORE DR\n",
       "90898         13792 LAKE SHORE DRIVE\n",
       "100792          1820 LAKEVIEW AVENUE\n",
       "100906           1898 PENROSE AVENUE\n",
       "100907           1898 PENROSE AVENUE\n",
       "101968             2115 REYBURN ROAD\n",
       "101978            2093 WESTBURN ROAD\n",
       "101979            2093 WESTBURN ROAD\n",
       "101980             2089 WESTBURN AVE\n",
       "111780              3124 ALBION ROAD\n",
       "111781              3124 ALBION ROAD\n",
       "111794             14004 BECKET ROAD\n",
       "111795             14004 BECKET ROAD\n",
       "112508               3465 ASHBY ROAD\n",
       "112541               3472 MENLO ROAD\n",
       "112629    15516 SCOTTSDALE BOULEVARD\n",
       "112737               3618 MENLO ROAD\n",
       "112738                 3606 MENLO RD\n",
       "112739                 3598 MENLO RD\n",
       "112740                 3598 MENLO RD\n",
       "112741               3590 MENLO ROAD\n",
       "112809               3582 MENLO ROAD\n",
       "112810               3570 MENLO ROAD\n",
       "Name: parcel_address, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.parcel_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fc2 = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_dec2014.csv', parse_dates = [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheriff auction\n",
    "Filename: ```shf_aution_mar2000_dec2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sa = pd.read_csv(path+'/data/original_data/shf_aution_mar2000_dec2014.csv', parse_dates=[2], encoding=\"ISO-8859-1\")\n",
    "\n",
    "sa = sa[sa.parcel.isin(ppns)]\n",
    "# sa = sa[sa.salesdt<np.datetime64('2015-06-01')]\n",
    "\n",
    "sa.to_csv(path+'/data/clean_data/sheriff_auction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfers\n",
    "Filename: ```transfers2000_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/transfers2000_2014.csv'\n",
    "outfile = path+'/data/clean_data/transfers.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(tf.mdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf = pd.read_csv(path+'/data/clean_data/transfers.csv',parse_dates=[8], dtype='str')\n",
    "# tf = tf[tf.mdate<np.datetime64('2015-06-01')]\n",
    "\n",
    "# tf.to_csv(path+'/data/clean_data/transfers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armslength sales\n",
    "Filename: ```armslengthsales2006_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print al.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)\n",
    "al = al[al.PROPERTY_NUMBER.isin(ppns)]\n",
    "al.to_csv(path+'/data/clean_data/armslength.csv',index=False)\n",
    "\n",
    "# al = pd.read_csv(path+'/data/clean_data/armslength.csv', parse_dates=['mdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations\n",
    "Filename: ```violate_cle.csv```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/violate_cle.csv'\n",
    "outfile = path+'/data/clean_data/violations.csv'\n",
    "\n",
    "v = pd.read_csv(infile, dtype=str)\n",
    "v = v[v.parcel.isin(ppns)]\n",
    "# v = v[(v.v_file_date < np.datetime64('2015-06-01')) & (v.v_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "v.to_csv(path+'/data/clean_data/violations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complaints\n",
    "Filename: ```complaint_cle.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = pd.read_csv(path+'/data/original_data/complaint_cle.csv', dtype=str)\n",
    "\n",
    "c = c[c.parcel.isin(ppns)]\n",
    "# c = c[(c.c_file_date < np.datetime64('2015-06-01')) & (c.c_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "c.to_csv(path+'/data/clean_data/complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal data\n",
    "Filenames: ```pv201302.csv, pv201304.csv, pv201308.csv, pv201312.csv, pv201402.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(p.PARCEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame()\n",
    "for pv in os.listdir(path+'/data/original_data/postal/'):\n",
    "    pos = pd.read_csv(path+'/data/original_data/postal/' + pv)\n",
    "    pos['date'] = dt.datetime(int(pv[2:6]), int(pv[6:8]), 1)\n",
    "    p = p.append(pos)\n",
    "p = p[p.PARCEL.isin(ppns)]\n",
    "p.to_csv(path+'/data/clean_data/postal_vacancy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
