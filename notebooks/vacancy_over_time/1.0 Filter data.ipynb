{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data used in vacancy model based on TCI parcel numbers\n",
    "\n",
    "Parcels surveyed in Summer 2015 so all data pulled should come from before that. Goal of script/notebook is to filter datasets by the parcel numbers in the TCI survey, although we will filter again based on existence of structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Dropbox/largetransfer/luc/carter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/data_sci/lib/python3.4/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/'.join(os.getcwd().split('/')[:-2])\n",
    "luc = '/'.join(os.getcwd().split('/')[:-3])\n",
    "print(path)\n",
    "\n",
    "tci = pd.read_csv(path+'/data/model_data/tci_1_0.csv', parse_dates=['Date'], dtype={'Ward':object,'PIN':str})\n",
    "ppns = set(tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].parcel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tci = pd.read_excel(path+'/data/inspection_data/Cleveland_Final_Results_Table_FOR_DISTRIBUTION_20151111.xlsx', encoding=\"ISO-8859-1\") \n",
    "\n",
    "# def get_vacant(x):\n",
    "#     if x == 'Occupied Structure':\n",
    "#         return 0\n",
    "#     elif x == 'Vacant Structure':\n",
    "#         return 1\n",
    "#     else: \n",
    "#         return -1\n",
    "    \n",
    "# tci['vacant'] = tci['Survey Category'].apply(get_vacant)\n",
    "# tci['parcel'] = tci.PIN.apply(lambda x: x[0:3]+'-'+x[3:5]+'-'+x[5:])\n",
    "\n",
    "# # tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].to_csv(path+'/data/model_data/tci_1_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(luc+'/demo.csv', parse_dates=['drdate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/data_sci/lib/python3.4/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# commented code only needs to be run once\n",
    "\n",
    "# main14 = main[main.taxyr=='2014'].groupby('parcel').first().reset_index()\n",
    "# main13 = main[main.taxyr=='2013'].groupby('parcel').first().reset_index()\n",
    "# main12 = main[main.taxyr=='2012'].groupby('parcel').first().reset_index()\n",
    "# main11 = main[main.taxyr=='2011'].groupby('parcel').first().reset_index()\n",
    "# main10 = main[main.taxyr=='2010'].groupby('parcel').first().reset_index()\n",
    "# main09 = main[main.taxyr=='2009'].groupby('parcel').first().reset_index()\n",
    "# main08 = main[main.taxyr=='2008'].groupby('parcel').first().reset_index()\n",
    "# main07 = main[main.taxyr=='2007'].groupby('parcel').first().reset_index()\n",
    "\n",
    "# main14.to_csv(path+'/data/clean_data/main_prop_2014.csv')\n",
    "# main13.to_csv(path+'/data/clean_data/main_prop_2013.csv')\n",
    "# main12.to_csv(path+'/data/clean_data/main_prop_2012.csv')\n",
    "# main11.to_csv(path+'/data/clean_data/main_prop_2011.csv')\n",
    "# main10.to_csv(path+'/data/clean_data/main_prop_2010.csv')\n",
    "# main09.to_csv(path+'/data/clean_data/main_prop_2009.csv')\n",
    "# main08.to_csv(path+'/data/clean_data/main_prop_2008.csv')\n",
    "# main07.to_csv(path+'/data/clean_data/main_prop_2007.csv')\n",
    "\n",
    "\n",
    "main14 = pd.read_csv(path+'/data/clean_data/main_prop_2014.csv')\n",
    "main13 = pd.read_csv(path+'/data/clean_data/main_prop_2013.csv')\n",
    "main12 = pd.read_csv(path+'/data/clean_data/main_prop_2012.csv')\n",
    "main11 = pd.read_csv(path+'/data/clean_data/main_prop_2011.csv')\n",
    "main10 = pd.read_csv(path+'/data/clean_data/main_prop_2010.csv')\n",
    "main09 = pd.read_csv(path+'/data/clean_data/main_prop_2009.csv')\n",
    "main08 = pd.read_csv(path+'/data/clean_data/main_prop_2008.csv')\n",
    "main07 = pd.read_csv(path+'/data/clean_data/main_prop_2007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main_all_years = main[main.taxyr.isin(['2007','2008','2009','2010','2011','2012','2013','2014','2015'])]\n",
    "main_all_years = main14.append(main13).append(main12).append(main11).append(main10).append(main09).append(main08).append(main07)\n",
    "main_all_years[['parcel','taxyr','pclass','propsize','front','depth','totbldgs','condition',\\\n",
    "        'totusabl','yrbuilt','style','tmktval','mktland','mktbldg','ownerocc','LATITUDE','LONGITUDE']]\\\n",
    "        .to_csv(path+'/data/clean_data/main_prop_all_years.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residential characteristics \n",
    "Filename: ```res2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first part only needs to be run once\n",
    "\n",
    "res2014 = pd.read_csv(luc+'/reschar/res2014.csv')\n",
    "res2013 = pd.read_csv(luc+'/reschar/res2013.csv')\n",
    "res2012 = pd.read_csv(luc+'/reschar/res2012.csv')\n",
    "res2011 = pd.read_csv(luc+'/reschar/res2011.csv')\n",
    "res2010 = pd.read_csv(luc+'/reschar/res2010.csv')\n",
    "res2009 = pd.read_csv(luc+'/reschar/res2009.csv')\n",
    "res2008 = pd.read_csv(luc+'/reschar/res2008.csv')\n",
    "res2007 = pd.read_csv(luc+'/reschar/res2007.csv')\n",
    "# res = res[res.parcel.isin(ppns)]\n",
    "# res.to_csv(path+'/data/clean_data/res.csv', index=False)\n",
    "\n",
    "# res = pd.read_csv(path+'/data/clean_data/res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res2014['year'] = 2014\n",
    "res2013['year'] = 2013\n",
    "res2012['year'] = 2012\n",
    "res2011['year'] = 2011\n",
    "res2010['year'] = 2010\n",
    "res2009['year'] = 2009\n",
    "res2008['year'] = 2008\n",
    "res2007['year'] = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reschar = res2014.loc[res2014.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']]\\\n",
    "            .append(res2013.loc[res2013.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2012.loc[res2012.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2011.loc[res2011.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2010.loc[res2010.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2009.loc[res2009.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2008.loc[res2008.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2007.loc[res2007.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reschar.to_csv(path+'/data/clean_data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax bill\n",
    "Filename: ```dec14_tci.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "\n",
    "cols = ['PROPERTY_NUMBER','TOTAL_NET_DELQ_BALANCE','TAX_ASSESSED_LAND','TAX_MARKET_LAND',\\\n",
    "        'LENDER_PROCESS_TYPE','GRAND_TOTAL_BALANCE','GRAND_TOTAL_PAID','GRAND_TOTAL_OWED']\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = ['dec14.csv', 'sep14.csv','jun14.csv','feb14.csv','nov13.csv','aug13.csv','apr13.csv',\\\n",
    "         'feb13.csv','dec12.csv','aug12.csv','jun12.csv','feb12.csv','sep11.csv','may11.csv',\\\n",
    "         'feb11.csv','nov10.csv','oct10.csv','jul10.csv','apr10.csv','oct09.csv','dec15.csv',\\\n",
    "         'oct15.csv','jul15.csv','may15.csv','feb15.csv']\n",
    "i = 1\n",
    "for file in files:\n",
    "    if i == 1:\n",
    "        tb = pd.read_csv(path+'/data/clean_data/taxbill/'+file, index_col=[0])\n",
    "    else:\n",
    "        tb2 = pd.read_csv(path+'/data/clean_data/taxbill/'+file, index_col=[0])\n",
    "        tb = tb.append(tb2)\n",
    "    i+=1\n",
    "tb.to_csv(path+'/data/clean_data/taxbill/tb_all.csv',index=False)\n",
    "#     infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "#     outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "#     tb = tb[cols]\n",
    "#     tb = tb[tb.PROPERTY_NUMBER.isin(ppns)]\n",
    "#     tb['date'] = dt.date(int('20'+file[3:5]), months[file[0:3]],1)\n",
    "#     tb.to_csv(path+'/data/clean_data/taxbill/'+file)\n",
    "    \n",
    "#     break\n",
    "#     with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "#         write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#         header = next(csv.reader(fin))\n",
    "#         write_to.writerow(header)\n",
    "#         for row in csv.reader(fin):\n",
    "#             if row[5] in ppns:\n",
    "#                 write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##only needs to be run once to clean taxbill data\n",
    "\n",
    "# infile = path+'/data/original_data/taxbill/sep14.csv'\n",
    "# outfile = path+'/data/clean_data/taxbill_sep14.csv'\n",
    "\n",
    "# with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "#     write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#     header = next(csv.reader(fin))\n",
    "#     write_to.writerow(header)\n",
    "#     for row in csv.reader(fin):\n",
    "#         if row[5] in ppns:\n",
    "#             write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## County land bank\n",
    "Filename: ```count_land_bank.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = pd.read_csv(path+'/data/original_data/count_land_bank.csv', parse_dates=[3,4])\n",
    "\n",
    "lb = lb[lb['parcel'].isin(ppns)]\n",
    "# lb = lb[lb['acq_dt']<np.datetime64('2015-06-01')]\n",
    "\n",
    "lb.to_csv(path+'/data/clean_data/county_lb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreclosure filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_beyond.csv', parse_dates = [2])\n",
    "\n",
    "fc = fc[fc['parcel'].isin(ppns)]\n",
    "# fc = fc[fc['filedate']<np.datetime64('2015-06-01')]\n",
    "\n",
    "fc.to_csv(path+'/data/clean_data/foreclosure_filings2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fc2 = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_dec2014.csv', parse_dates = [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheriff auction\n",
    "Filename: ```shf_aution_mar2000_dec2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sa = pd.read_csv(path+'/data/original_data/shf_aution_mar2000_dec2014.csv', parse_dates=[2], encoding=\"ISO-8859-1\")\n",
    "\n",
    "sa = sa[sa.parcel.isin(ppns)]\n",
    "# sa = sa[sa.salesdt<np.datetime64('2015-06-01')]\n",
    "\n",
    "sa.to_csv(path+'/data/clean_data/sheriff_auction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfers\n",
    "Filename: ```transfers2000_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/transfers2000_beyond.csv'\n",
    "outfile = path+'/data/clean_data/transfers.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf = pd.read_csv(path+'/data/clean_data/transfers.csv',parse_dates=[8], dtype='str')\n",
    "# # tf = tf[tf.mdate<np.datetime64('2015-06-01')]\n",
    "\n",
    "# tf.to_csv(path+'/data/clean_data/transfers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armslength sales\n",
    "Filename: ```armslengthsales2006_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)\n",
    "al = al[al.PROPERTY_NUMBER.isin(ppns)]\n",
    "al.to_csv(path+'/data/clean_data/armslength.csv',index=False)\n",
    "\n",
    "# al = pd.read_csv(path+'/data/clean_data/armslength.csv', parse_dates=['mdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations\n",
    "Filename: ```violate_cle.csv```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/violate_cle.csv'\n",
    "outfile = path+'/data/clean_data/violations.csv'\n",
    "\n",
    "v = pd.read_csv(infile, dtype=str)\n",
    "v = v[v.parcel.isin(ppns)]\n",
    "# v = v[(v.v_file_date < np.datetime64('2015-06-01')) & (v.v_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "# v.to_csv(path+'/data/clean_data/violations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complaints\n",
    "Filename: ```complaint_cle.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = pd.read_csv(path+'/data/original_data/complaint_cle.csv', dtype=str)\n",
    "\n",
    "c = c[c.parcel.isin(ppns)]\n",
    "# c = c[(c.c_file_date < np.datetime64('2015-06-01')) & (c.c_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "# c.to_csv(path+'/data/clean_data/complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal data\n",
    "Filenames: ```pv201302.csv, pv201304.csv, pv201308.csv, pv201312.csv, pv201402.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113132, 33)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tci.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pv200807.csv\n",
      "pv200908.csv\n",
      "pv200909.csv\n",
      "pv201001.csv\n",
      "pv201003.csv\n",
      "pv201005.csv\n",
      "pv201007.csv\n",
      "pv201009.csv\n",
      "pv201101.csv\n",
      "pv201103.csv\n",
      "pv201106.csv\n",
      "pv201110.csv\n",
      "pv201112.csv\n",
      "pv201202.csv\n",
      "pv201204.csv\n",
      "pv201206.csv\n",
      "pv201210.csv\n",
      "pv201212.csv\n",
      "pv201302.csv\n",
      "pv201304.csv\n",
      "pv201308.csv\n",
      "pv201312.csv\n",
      "pv201402.csv\n",
      "pv201406.csv\n",
      "pv201410.csv\n",
      "pv201501.csv\n",
      "pv201502.csv\n",
      "pv201504.csv\n",
      "pv201509.csv\n",
      "pv201510.csv\n",
      "pv201512.csv\n"
     ]
    }
   ],
   "source": [
    "p = pd.DataFrame()\n",
    "for pv in os.listdir(path+'/data/original_data/postal/'):\n",
    "    print(pv)\n",
    "    pos = pd.read_csv(path+'/data/original_data/postal/' + pv)\n",
    "    pos['date'] = dt.datetime(int(pv[2:6]), int(pv[6:8]), 1)\n",
    "    p = p.append(pos)\n",
    "p = p[p.PARCEL.isin(ppns)]\n",
    "p.to_csv(path+'/data/clean_data/postal_vacancy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
