{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data used in vacancy model based on TCI parcel numbers\n",
    "\n",
    "Parcels surveyed in Summer 2015 so all data pulled should come from before that. Goal of script/notebook is to filter datasets by the parcel numbers in the TCI survey, although we will filter again based on existence of structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Dropbox/largetransfer/luc/carter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/'.join(os.getcwd().split('/')[:-2])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci = pd.read_excel(path+'/data/inspection_data/Cleveland_Final_Results_Table_FOR_DISTRIBUTION_20151111.xlsx', encoding=\"ISO-8859-1\") \n",
    "\n",
    "def get_vacant(x):\n",
    "    if x == 'Occupied Structure':\n",
    "        return 0\n",
    "    elif x == 'Vacant Structure':\n",
    "        return 1\n",
    "    else: \n",
    "        return -1\n",
    "    \n",
    "tci['vacant'] = tci['Survey Category'].apply(get_vacant)\n",
    "tci['parcel'] = tci.PIN.apply(lambda x: x[0:3]+'-'+x[3:5]+'-'+x[5:])\n",
    "\n",
    "# tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].to_csv(path+'/data/model_data/tci_1_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(tci[tci.vacant>0].vacant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci[(tci.USE_CLASS=='R')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci = pd.read_csv(path+'/data/model_data/tci_1_0.csv', parse_dates=['Date'], dtype={'Ward':object,'PIN':str})\n",
    "ppns = set(tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tracts = pd.read_csv(path+'/data/original_data/clv_par_census.csv')\n",
    "# demo = pd.read_csv(path+'/data/original_data/sociodemographic_Data.csv')\n",
    "\n",
    "# tracts = pd.merge(tracts, demo, left_on='NAME10', right_on='Census Tract', how='left')\n",
    "# cols = [0,1,5,7,9,11,13,14,15,16,17,18,20,22,24,26,28,30,32,34,36,38,40]\n",
    "# tracts.iloc[:,cols].to_csv(path+'/data/clean_data/demographic.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path+'/data/original_data/main_prop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dbf = simpledbf.Dbf5(path+'/data/original_data/parcel0611_lookup2010.dbf').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dbf[dbf.CITY=='CLEVELAND'].groupby('PARCEL').first().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# infile = path+'/data/original_data/main_prop.csv'\n",
    "# outfile = path+'/data/clean_data/main_prop13.csv'\n",
    "\n",
    "# with open(infile, 'r', encoding=\"ISO-8859-1\") as fin, open(outfile, 'w') as fout:\n",
    "#     write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#     header = next(csv.reader(fin))\n",
    "#     write_to.writerow(header)\n",
    "#     for row in csv.reader(fin):\n",
    "#         if row[0] in ppns:\n",
    "#             write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# commented code only needs to be run once\n",
    "\n",
    "main = pd.read_csv(path+'/data/clean_data/main_prop13.csv', dtype=object)\n",
    "\n",
    "main = main.drop_duplicates()\n",
    "main = main[main.parcel.isin(ppns)]\n",
    "\n",
    "main14 = main[main.taxyr=='2014'].groupby('parcel').first().reset_index()\n",
    "main13 = main[main.taxyr=='2013'].groupby('parcel').first().reset_index()\n",
    "\n",
    "print(main13.shape, main14.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main13 = main13.set_index('parcel')\n",
    "main13 = main13.rename(columns={'condition':'condition13'})\n",
    "main14 = main14.rename(columns={'condition':'condition14'})\n",
    "pd.merge(main14, main13[['condition13']], how='left', left_on='parcel', right_index=True)\\\n",
    "    .to_csv(path+'/data/clean_data/main_prop_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residential characteristics \n",
    "Filename: ```res2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425302, 46)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('parcel').count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first part only needs to be run once\n",
    "\n",
    "res = pd.read_csv(path+'/data/original_data/res/res2014.csv')\n",
    "# res = res[res.parcel.isin(ppns)]\n",
    "# res.to_csv(path+'/data/clean_data/res.csv', index=False)\n",
    "\n",
    "# res = pd.read_csv(path+'/data/clean_data/res.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax bill\n",
    "Filename: ```dec14_tci.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (4,20,22,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_csv(path+'/data/clean_data/taxbill_may15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/sep14.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_sep14.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## County land bank\n",
    "Filename: ```count_land_bank.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = pd.read_csv(path+'/data/original_data/count_land_bank.csv', parse_dates=[3,4])\n",
    "\n",
    "lb = lb[lb['parcel'].isin(ppns)]\n",
    "# lb = lb[lb['acq_dt']<np.datetime64('2015-06-01')]\n",
    "\n",
    "lb.to_csv(path+'/data/clean_data/county_lb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreclosure filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_beyond.csv', parse_dates = [2])\n",
    "\n",
    "fc = fc[fc['parcel'].isin(ppns)]\n",
    "# fc = fc[fc['filedate']<np.datetime64('2015-06-01')]\n",
    "\n",
    "fc.to_csv(path+'/data/clean_data/foreclosure_filings2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fc2 = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_dec2014.csv', parse_dates = [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheriff auction\n",
    "Filename: ```shf_aution_mar2000_dec2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sa = pd.read_csv(path+'/data/original_data/shf_aution_mar2000_dec2014.csv', parse_dates=[2], encoding=\"ISO-8859-1\")\n",
    "\n",
    "sa = sa[sa.parcel.isin(ppns)]\n",
    "# sa = sa[sa.salesdt<np.datetime64('2015-06-01')]\n",
    "\n",
    "sa.to_csv(path+'/data/clean_data/sheriff_auction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfers\n",
    "Filename: ```transfers2000_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/transfers2000_2014.csv'\n",
    "outfile = path+'/data/clean_data/transfers.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(tf.mdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf = pd.read_csv(path+'/data/clean_data/transfers.csv',parse_dates=[8], dtype='str')\n",
    "# tf = tf[tf.mdate<np.datetime64('2015-06-01')]\n",
    "\n",
    "# tf.to_csv(path+'/data/clean_data/transfers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armslength sales\n",
    "Filename: ```armslengthsales2006_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print al.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)\n",
    "al = al[al.PROPERTY_NUMBER.isin(ppns)]\n",
    "al.to_csv(path+'/data/clean_data/armslength.csv',index=False)\n",
    "\n",
    "# al = pd.read_csv(path+'/data/clean_data/armslength.csv', parse_dates=['mdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations\n",
    "Filename: ```violate_cle.csv```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/violate_cle.csv'\n",
    "outfile = path+'/data/clean_data/violations.csv'\n",
    "\n",
    "v = pd.read_csv(infile, dtype=str)\n",
    "v = v[v.parcel.isin(ppns)]\n",
    "# v = v[(v.v_file_date < np.datetime64('2015-06-01')) & (v.v_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "v.to_csv(path+'/data/clean_data/violations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complaints\n",
    "Filename: ```complaint_cle.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = pd.read_csv(path+'/data/original_data/complaint_cle.csv', dtype=str)\n",
    "\n",
    "c = c[c.parcel.isin(ppns)]\n",
    "# c = c[(c.c_file_date < np.datetime64('2015-06-01')) & (c.c_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "c.to_csv(path+'/data/clean_data/complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal data\n",
    "Filenames: ```pv201302.csv, pv201304.csv, pv201308.csv, pv201312.csv, pv201402.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(p.PARCEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame()\n",
    "for pv in os.listdir(path+'/data/original_data/postal/'):\n",
    "    pos = pd.read_csv(path+'/data/original_data/postal/' + pv)\n",
    "    pos['date'] = dt.datetime(int(pv[2:6]), int(pv[6:8]), 1)\n",
    "    p = p.append(pos)\n",
    "p = p[p.PARCEL.isin(ppns)]\n",
    "p.to_csv(path+'/data/clean_data/postal_vacancy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
