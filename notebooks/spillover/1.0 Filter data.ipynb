{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter data used in vacancy model based on TCI parcel numbers\n",
    "\n",
    "Parcels surveyed in Summer 2015 so all data pulled should come from before that. Goal of script/notebook is to filter datasets by the parcel numbers in the TCI survey, although we will filter again based on existence of structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Dropbox/largetransfer/luc/carter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/'.join(os.getcwd().split('/')[:-2])\n",
    "luc = '/'.join(os.getcwd().split('/')[:-3])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tci = pd.read_excel(path+'/data/inspection_data/Cleveland_Final_Results_Table_FOR_DISTRIBUTION_20151111.xlsx', encoding=\"ISO-8859-1\") \n",
    "\n",
    "# def get_vacant(x):\n",
    "#     if x == 'Occupied Structure':\n",
    "#         return 0\n",
    "#     elif x == 'Vacant Structure':\n",
    "#         return 1\n",
    "#     else: \n",
    "#         return -1\n",
    "    \n",
    "# tci['vacant'] = tci['Survey Category'].apply(get_vacant)\n",
    "# tci['parcel'] = tci.PIN.apply(lambda x: x[0:3]+'-'+x[3:5]+'-'+x[5:])\n",
    "\n",
    "# # tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].to_csv(path+'/data/model_data/tci_1_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tci = pd.read_csv(path+'/data/model_data/tci_1_0.csv', parse_dates=['Date'], dtype={'Ward':object,'PIN':str})\n",
    "ppns = set(tci[(tci.USE_CLASS=='R') & (tci.vacant>-1)].parcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tracts = pd.read_csv(path+'/data/original_data/clv_par_census.csv')\n",
    "# demo = pd.read_csv(path+'/data/original_data/sociodemographic_Data.csv')\n",
    "\n",
    "# tracts = pd.merge(tracts, demo, left_on='NAME10', right_on='Census Tract', how='left')\n",
    "# cols = [0,1,5,7,9,11,13,14,15,16,17,18,20,22,24,26,28,30,32,34,36,38,40]\n",
    "# tracts.iloc[:,cols].to_csv(path+'/data/clean_data/demographic.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Property characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(path+'/data/original_data/main_prop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import simpledbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dbf = simpledbf.Dbf5(path+'/data/original_data/parcel0611_lookup2010.dbf').to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dbf[dbf.CITY=='CLEVELAND'].groupby('PARCEL').first().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# infile = path+'/data/original_data/main_prop.csv'\n",
    "# outfile = path+'/data/clean_data/main_prop13.csv'\n",
    "\n",
    "# with open(infile, 'r', encoding=\"ISO-8859-1\") as fin, open(outfile, 'w') as fout:\n",
    "#     write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#     header = next(csv.reader(fin))\n",
    "#     write_to.writerow(header)\n",
    "#     for row in csv.reader(fin):\n",
    "#         if row[0] in ppns:\n",
    "#             write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main = pd.read_csv(path+'/data/clean_data/main_prop13.csv', dtype=object)\n",
    "\n",
    "main = main.drop_duplicates()\n",
    "main = main[main.parcel.isin(ppns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113092, 65) (113117, 65)\n"
     ]
    }
   ],
   "source": [
    "# commented code only needs to be run once\n",
    "\n",
    "main14 = main[main.taxyr=='2014'].groupby('parcel').first().reset_index()\n",
    "main13 = main[main.taxyr=='2013'].groupby('parcel').first().reset_index()\n",
    "main12 = main[main.taxyr=='2012'].groupby('parcel').first().reset_index()\n",
    "main11 = main[main.taxyr=='2011'].groupby('parcel').first().reset_index()\n",
    "main10 = main[main.taxyr=='2010'].groupby('parcel').first().reset_index()\n",
    "main09 = main[main.taxyr=='2009'].groupby('parcel').first().reset_index()\n",
    "main08 = main[main.taxyr=='2008'].groupby('parcel').first().reset_index()\n",
    "main07 = main[main.taxyr=='2007'].groupby('parcel').first().reset_index()\n",
    "\n",
    "main14.to_csv(path+'/data/clean_data/main_prop_2014.csv')\n",
    "main13.to_csv(path+'/data/clean_data/main_prop_2013.csv')\n",
    "main12.to_csv(path+'/data/clean_data/main_prop_2012.csv')\n",
    "main11.to_csv(path+'/data/clean_data/main_prop_2011.csv')\n",
    "main10.to_csv(path+'/data/clean_data/main_prop_2010.csv')\n",
    "main09.to_csv(path+'/data/clean_data/main_prop_2009.csv')\n",
    "main08.to_csv(path+'/data/clean_data/main_prop_2008.csv')\n",
    "main07.to_csv(path+'/data/clean_data/main_prop_2007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_all_years = main[main.taxyr.isin(['2007','2008','2009','2010','2011','2012','2013','2014','2015'])]\\\n",
    "            .groupby('parcel').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main_all_years[['parcel','taxyr','pclass','propsize','front','depth','totbldgs','condition',\\\n",
    "        'totusabl','yrbuilt','style','tmktval','mktland','mktbldg','ownerocc','LATITUDE','LONGITUDE']]\\\n",
    "        .to_csv(path+'/data/clean_data/main_prop_all_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main13 = main13.set_index('parcel')\n",
    "# main13 = main13.rename(columns={'condition':'condition13'})\n",
    "# main14 = main14.rename(columns={'condition':'condition14'})\n",
    "# pd.merge(main14, main13[['condition13']], how='left', left_on='parcel', right_index=True)\\\n",
    "#     .to_csv(path+'/data/clean_data/main_prop_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residential characteristics \n",
    "Filename: ```res2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(425302, 46)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('parcel').count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first part only needs to be run once\n",
    "\n",
    "res2014 = pd.read_csv(luc+'/reschar/res2014.csv')\n",
    "res2013 = pd.read_csv(luc+'/reschar/res2013.csv')\n",
    "res2012 = pd.read_csv(luc+'/reschar/res2012.csv')\n",
    "res2011 = pd.read_csv(luc+'/reschar/res2011.csv')\n",
    "res2010 = pd.read_csv(luc+'/reschar/res2010.csv')\n",
    "res2009 = pd.read_csv(luc+'/reschar/res2009.csv')\n",
    "res2008 = pd.read_csv(luc+'/reschar/res2008.csv')\n",
    "res2007 = pd.read_csv(luc+'/reschar/res2007.csv')\n",
    "# res = res[res.parcel.isin(ppns)]\n",
    "# res.to_csv(path+'/data/clean_data/res.csv', index=False)\n",
    "\n",
    "# res = pd.read_csv(path+'/data/clean_data/res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res2014['year'] = 2014\n",
    "res2013['year'] = 2013\n",
    "res2012['year'] = 2012\n",
    "res2011['year'] = 2011\n",
    "res2010['year'] = 2010\n",
    "res2009['year'] = 2009\n",
    "res2008['year'] = 2008\n",
    "res2007['year'] = 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reschar = res2014.loc[res2014.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']]\\\n",
    "            .append(res2013.loc[res2013.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2012.loc[res2012.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2011.loc[res2011.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2010.loc[res2010.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2009.loc[res2009.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2008.loc[res2008.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\\\n",
    "            .append(res2007.loc[res2007.parcel.isin(ppns),['parcel','cqual','occup','rnumstor','year']])\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reschar.to_csv(path+'/data/clean_data/res.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax bill\n",
    "Filename: ```dec14_tci.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb7 = pd.read_csv(luc+'/taxbill/tax2007.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['PROPERTY_NUMBER','TOTAL_NET_DELQ_BALANCE','TAX_ASSESSED_LAND','TAX_MARKET_LAND',\\\n",
    "        'LENDER_PROCESS_TYPE','GRAND_TOTAL_BALANCE','GRAND_TOTAL_PAID','GRAND_TOTAL_OWED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(553027, 70)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcel', 'TAXYEAR', 'TAXSET', 'TAXDESCR', 'PCLASSDESC', 'OWNER1',\n",
       "       'OWNER2', 'DIR', 'HNUM', 'STNAME', 'SUFFIX', 'CITY', 'STATE', 'ZIP',\n",
       "       'MAILNAME', 'MAILHNUM', 'MAILDIR', 'MAILSTNAME', 'MAILSUFFIX',\n",
       "       'MAILATTN', 'mailsecondary', 'MAILCITY', 'MAILSTATE', 'mailcountry',\n",
       "       'MAILZIP', 'MKTLAND', 'MKTBLDG', 'MKTVAL', 'LVAL', 'BVAL', 'TOTVAL',\n",
       "       'HMVALRED', 'GROSSRATE', 'REDFACTOR', 'EFFRATE', 'BANKID', 'DELQTAX',\n",
       "       'DELQSPA', 'DELQGTAX', 'DELQTAXPAY', 'DELQSPAPAY', 'DELQNETPAY',\n",
       "       'taxinterest', 'spainterest', 'netinterest', 'GROSSTAX_1', 'REDCRED_1',\n",
       "       'SUBTOTAL_1', 'ROLLAMT_1', 'RBAOWNOC_1', 'HMTXCR_1', 'OMITTED_1',\n",
       "       'taxpenalty_1', 'spapenalty_1', 'netpenalty_1', 'NETTAX_1', 'NETSPA_1',\n",
       "       'NETCURR_1', 'contractamt_1', 'RTAXPAY_1', 'spataxpay_1', 'nettaxpay_1',\n",
       "       'escrowpay_1', 'TOTDUE_1', 'installdue_1', 'book', 'page', 'item',\n",
       "       'pclass', 'luc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2014, 12, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'dec14.csv'\n",
    "int('20'+file[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(559876, 9)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (4,6,7,13,20,22,27,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (6,7,13,20,22,27,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,4,5,6,7,9,10,13,14,15,16,17,18,19,20,21,22,23,42,44,45,74,111,114) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,6,7,9,10,14,16,17,19,20,21,22,23,42,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,4,5,6,7,9,10,13,14,15,16,17,18,19,20,21,22,23,44,45,74,111,114) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,6,7,9,10,14,16,17,19,21,22,23,42,44,45) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (1,4,5,6,7,9,10,13,14,15,16,17,18,19,20,21,22,23,42,44,45,74,111,114,142) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (6,7,13,20,27,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "files = ['dec14.csv', 'sep14.csv','jun14.csv','feb14.csv','nov13.csv','aug13.csv','apr13.csv',\\\n",
    "         'feb13.csv','dec12.csv','aug12.csv','jun12.csv','feb12.csv','sep11.csv','may11.csv',\\\n",
    "         'feb11.csv','nov10.csv','oct10.csv','jul10.csv','apr10.csv','oct09.csv','dec15.csv',\\\n",
    "         'oct15.csv','jul15.csv','may15.csv','feb15.csv']\n",
    "taxbill_all = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    tb = pd.read_csv(luc+'/taxbill/'+file)\n",
    "#     infile = path+'/data/original_data/taxbill/may15.csv'\n",
    "#     outfile = path+'/data/clean_data/taxbill_may15.csv'\n",
    "    tb = tb[cols]\n",
    "    tb = tb[tb.PROPERTY_NUMBER.isin(ppns)]\n",
    "    tb['date'] = dt.date(int('20'+file[3:5]), months[file[0:3]],1)\n",
    "    tb.to_csv(path+'/data/clean_data/taxbill/'+file)\n",
    "#     taxbill_all.append(tb)\n",
    "#     break\n",
    "#     with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "#         write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#         header = next(csv.reader(fin))\n",
    "#         write_to.writerow(header)\n",
    "#         for row in csv.reader(fin):\n",
    "#             if row[5] in ppns:\n",
    "#                 write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (4,20,22,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "tb = pd.read_csv(path+'/data/clean_data/taxbill_may15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##only needs to be run once to clean taxbill data\n",
    "\n",
    "infile = path+'/data/original_data/taxbill/sep14.csv'\n",
    "outfile = path+'/data/clean_data/taxbill_sep14.csv'\n",
    "\n",
    "with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "    write_to = csv.writer(fout, lineterminator='\\n')\n",
    "    header = next(csv.reader(fin))\n",
    "    write_to.writerow(header)\n",
    "    for row in csv.reader(fin):\n",
    "        if row[5] in ppns:\n",
    "            write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## County land bank\n",
    "Filename: ```count_land_bank.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb = pd.read_csv(path+'/data/original_data/count_land_bank.csv', parse_dates=[3,4])\n",
    "\n",
    "lb = lb[lb['parcel'].isin(ppns)]\n",
    "# lb = lb[lb['acq_dt']<np.datetime64('2015-06-01')]\n",
    "\n",
    "lb.to_csv(path+'/data/clean_data/county_lb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreclosure filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_beyond.csv', parse_dates = [2])\n",
    "\n",
    "fc = fc[fc['parcel'].isin(ppns)]\n",
    "# fc = fc[fc['filedate']<np.datetime64('2015-06-01')]\n",
    "\n",
    "fc.to_csv(path+'/data/clean_data/foreclosure_filings2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fc2 = pd.read_csv(path+'/data/original_data/foreclosure_filings2006_dec2014.csv', parse_dates = [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheriff auction\n",
    "Filename: ```shf_aution_mar2000_dec2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sa = pd.read_csv(path+'/data/original_data/shf_aution_mar2000_dec2014.csv', parse_dates=[2], encoding=\"ISO-8859-1\")\n",
    "\n",
    "sa = sa[sa.parcel.isin(ppns)]\n",
    "# sa = sa[sa.salesdt<np.datetime64('2015-06-01')]\n",
    "\n",
    "sa.to_csv(path+'/data/clean_data/sheriff_auction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfers\n",
    "Filename: ```transfers2000_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# infile = path+'/data/original_data/transfers2000_2014.csv'\n",
    "# outfile = path+'/data/clean_data/transfers.csv'\n",
    "\n",
    "# with open(infile, 'r') as fin, open(outfile, 'w') as fout:\n",
    "#     write_to = csv.writer(fout, lineterminator='\\n')\n",
    "#     header = next(csv.reader(fin))\n",
    "#     write_to.writerow(header)\n",
    "#     for row in csv.reader(fin):\n",
    "#         if row[5] in ppns:\n",
    "#             write_to.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf = pd.read_csv(path+'/data/clean_data/transfers.csv',parse_dates=[8], dtype='str')\n",
    "# tf = tf[tf.mdate<np.datetime64('2015-06-01')]\n",
    "\n",
    "# tf.to_csv(path+'/data/clean_data/transfers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Armslength sales\n",
    "Filename: ```armslengthsales2006_2014.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print al.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "al = pd.read_csv(path+'/data/original_data/armslengthsales2006_beyond.csv', dtype=str)\n",
    "al = al[al.PROPERTY_NUMBER.isin(ppns)]\n",
    "al.to_csv(path+'/data/clean_data/armslength.csv',index=False)\n",
    "\n",
    "# al = pd.read_csv(path+'/data/clean_data/armslength.csv', parse_dates=['mdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violations\n",
    "Filename: ```violate_cle.csv```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "infile = path+'/data/original_data/violate_cle.csv'\n",
    "outfile = path+'/data/clean_data/violations.csv'\n",
    "\n",
    "v = pd.read_csv(infile, dtype=str)\n",
    "v = v[v.parcel.isin(ppns)]\n",
    "# v = v[(v.v_file_date < np.datetime64('2015-06-01')) & (v.v_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "# v.to_csv(path+'/data/clean_data/violations.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complaints\n",
    "Filename: ```complaint_cle.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = pd.read_csv(path+'/data/original_data/complaint_cle.csv', dtype=str)\n",
    "\n",
    "c = c[c.parcel.isin(ppns)]\n",
    "# c = c[(c.c_file_date < np.datetime64('2015-06-01')) & (c.c_file_date > np.datetime64('2006-06-01'))]\n",
    "\n",
    "# c.to_csv(path+'/data/clean_data/complaints.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal data\n",
    "Filenames: ```pv201302.csv, pv201304.csv, pv201308.csv, pv201312.csv, pv201402.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(p.PARCEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pd.DataFrame()\n",
    "for pv in os.listdir(path+'/data/original_data/postal/'):\n",
    "    pos = pd.read_csv(path+'/data/original_data/postal/' + pv)\n",
    "    pos['date'] = dt.datetime(int(pv[2:6]), int(pv[6:8]), 1)\n",
    "    p = p.append(pos)\n",
    "p = p[p.PARCEL.isin(ppns)]\n",
    "p.to_csv(path+'/data/clean_data/postal_vacancy.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
